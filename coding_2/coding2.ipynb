{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Assignment 2\n",
    "\n",
    "Team:\n",
    "- Olivia Dalglish (od4)\n",
    "- Arindam Saha (saha2)\n",
    "\n",
    "Contribution: \n",
    "\n",
    "Olivia: Part 2\n",
    "\n",
    "Arindam: Part 1\n",
    "\n",
    "In addition to the above, we discussed our approaches and checked each other's work.\n",
    "\n",
    "### Part 1: Implement Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from tqdm.notebook import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData = pd.read_csv(\"Coding2_Data0.csv\")\n",
    "var_names = myData.columns\n",
    "y = myData[['Y']].to_numpy()\n",
    "X = myData.drop(['Y'], axis = 1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_var_lasso(v, z, lam):\n",
    "    v = v.ravel()\n",
    "    z = z.ravel()\n",
    "    ztz = z @ z\n",
    "    a = (v @ z) / ztz\n",
    "    eta = (2 * len(v) * lam) / ztz\n",
    "    return np.sign(a) * max(np.abs(a) - eta/2, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced code from https://liangfgithub.github.io/Coding/F24_Coding2_Part1_Python.html\n",
    "\n",
    "def MyLasso(X, y, lam_seq, maxit = 100):\n",
    "    \n",
    "    # Input\n",
    "    # X: n-by-p design matrix without the intercept \n",
    "    # y: n-by-1 response vector \n",
    "    # lam.seq: sequence of lambda values (arranged from large to small)\n",
    "    # maxit: number of updates for each lambda \n",
    "    \n",
    "    # Output\n",
    "    # B: a (p+1)-by-len(lam.seq) coefficient matrix \n",
    "    #    with the first row being the intercept sequence \n",
    "\n",
    "  \n",
    "    n, p = X.shape\n",
    "    nlam = len(lam_seq)\n",
    "    B = np.zeros((p+1, nlam))\n",
    "    \n",
    "    ##############################\n",
    "    # YOUR CODE: \n",
    "    # (1) newX = Standardizad X; \n",
    "    # (2) Record the centers and scales used in (1) \n",
    "    ##############################\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_sd = np.std(X, axis=0)\n",
    "    newX = (X - X_mean) / X_sd\n",
    "\n",
    "    # Initilize coef vector b and residual vector r\n",
    "    b = np.zeros(p)\n",
    "    r = y\n",
    "    \n",
    "    # Triple nested loop\n",
    "    for m in range(nlam):\n",
    "        for step in range(maxit):\n",
    "            for j in range(p):\n",
    "                X_j = newX[:, j].reshape(-1,1)\n",
    "                r = r + X_j * b[j]\n",
    "                b[j] = one_var_lasso(r, X_j, lam_seq[m])\n",
    "                r = r - X_j * b[j]\n",
    "        B[1:, m] = b \n",
    "    \n",
    "    ##############################\n",
    "    # YOUR CODE:\n",
    "    # Scale back the coefficients;\n",
    "    # Update the intercepts stored in B[0, ]\n",
    "    ##############################\n",
    "    B[0, :] = (B[1:, :].T @ (-X_mean / X_sd)) + np.mean(y)\n",
    "    B[1:, :] /= X_sd[:, np.newaxis]\n",
    "    \n",
    "    return(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lam_seq = np.linspace(-1, -8, num = 80)\n",
    "lam_seq = np.exp(log_lam_seq)\n",
    "myout = MyLasso(X, y, lam_seq, maxit = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, _ = myout.shape\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "for i in range(p-1):\n",
    "    plt.plot(log_lam_seq, myout[i+1, :], label = var_names[i])\n",
    "\n",
    "plt.xlabel('Log Lambda')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Lasso Paths - Numpy implementation')\n",
    "plt.legend()\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coef = pd.read_csv(\"Coding2_lasso_coefs.csv\").to_numpy()\n",
    "lasso_coef.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(myout - lasso_coef).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the the maximum difference between the two coefficient matrices is indeed less than 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0fd151",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression as lm\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "np.random.seed(1735)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169837a7-4c78-4c17-b016-12bfc38d7437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, Y):\n",
    "    \"\"\"Split data into training and testing partitions\"\"\"\n",
    "    n = len(Y)\n",
    "\n",
    "    indices = np.arange(0, n)\n",
    "    np.random.shuffle(indices)\n",
    "    test_ind = indices[:int(np.floor(0.25*n))]\n",
    "    train_ind = indices[len(test_ind):]\n",
    "    \n",
    "    X_train = X.iloc[train_ind]\n",
    "    Y_train = Y[train_ind]\n",
    "    X_test = X.iloc[test_ind]\n",
    "    Y_test = Y[test_ind]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def normalize(X_train, X_test):\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c20bf-471b-48bb-b87d-abd3e71dcdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(X, Y, models=[], niter=50):\n",
    "    results = defaultdict(list)\n",
    "    for i in range(niter):\n",
    "        runner = Predictor(X, Y) #initalize new predictor object - data will be resplit to train/test\n",
    "        for model_type in models:\n",
    "            model_run_results = runner.run_model(model_type)\n",
    "            for model_name, mse in model_run_results.items():\n",
    "                results[model_name].append(mse)\n",
    "    return results\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, X, Y):\n",
    "        X_train, Y_train, X_test, Y_test = train_test_split(X, Y)\n",
    "        X_train, X_test = normalize(X_train, X_test)\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train.to_numpy()\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test.to_numpy()\n",
    "\n",
    "    def run_model(self, model_type):\n",
    "        results = {}\n",
    "        fit_method = getattr(self, f\"fit_{model_type}\")\n",
    "        models = fit_method()\n",
    "        for model_name, model in models.items():\n",
    "            if model_name == \"lasso_model_refit\":\n",
    "                mse = self.mse(model, self.X_train[:, self.lasso_refit_nonzero_indices], self.Y_train)\n",
    "            else:\n",
    "                mse = self.mse(model, self.X_test, self.Y_test)\n",
    "            results[model_name] = mse\n",
    "        return results\n",
    "        \n",
    "    def mse(self, model, X_test, Y_test):\n",
    "        return mean_squared_error(Y_test, model.predict(X_test))\n",
    "    \n",
    "    def fit_linear(self):\n",
    "        model = lm().fit(self.X_train, self.Y_train)\n",
    "        return {\"linear_model\": model}\n",
    "\n",
    "    def fit_ridge(self):\n",
    "        ridge_alphas = np.logspace(-10, 1, 100)\n",
    "        ridgecv = RidgeCV(alphas = ridge_alphas, cv = 10, \n",
    "                          scoring = 'neg_mean_squared_error')\n",
    "        ridgecv.fit(self.X_train, self.Y_train)\n",
    "        ridgecv.alpha_\n",
    "        \n",
    "        ridge_model = Ridge(alpha = ridgecv.alpha_)\n",
    "        ridge_model.fit(self.X_train, self.Y_train)\n",
    "\n",
    "        return {\"ridge_model\": ridge_model}\n",
    "\n",
    "    def fit_lasso(self):\n",
    "        lasso_alphas = np.logspace(-10, 1, 100)\n",
    "        lassocv = LassoCV(alphas = lasso_alphas, cv = 10)\n",
    "        lassocv.fit(self.X_train, self.Y_train)\n",
    "        lassocv.alpha_\n",
    "        \n",
    "        mean_mse = np.mean(lassocv.mse_path_, axis=1)\n",
    "        std_mse = np.std(lassocv.mse_path_, axis=1) / np.sqrt(10) \n",
    "        \n",
    "        cv_alphas = lassocv.alphas_\n",
    "        min_idx = np.argmin(mean_mse)\n",
    "        \n",
    "        alpha_min = cv_alphas[min_idx]\n",
    "        \n",
    "        threshold = mean_mse[min_idx] + std_mse[min_idx]\n",
    "        alpha_1se = max(cv_alphas[np.where(mean_mse <= threshold)])\n",
    "        \n",
    "        alpha_min, alpha_1se  #alpha_min = lassocv.alpha_\n",
    "\n",
    "        # lasso model with min alpha\n",
    "        lasso_model_min = Lasso(alpha = alpha_min, max_iter=10000)\n",
    "        lasso_model_min.fit(self.X_train, self.Y_train)\n",
    "\n",
    "        # lasso model with 1se alpha\n",
    "        lasso_model_1se = Lasso(alpha = alpha_1se, max_iter=10000)\n",
    "        lasso_model_1se.fit(self.X_train, self.Y_train)\n",
    "\n",
    "        # lasso model refit with 1se\n",
    "        self.lasso_refit_nonzero_indices = np.where(lasso_model_1se.coef_ != 0)[0]\n",
    "        lm_refit = lm()\n",
    "        lm_refit.fit(self.X_train[:, self.lasso_refit_nonzero_indices], self.Y_train)\n",
    "        print(f\"Number of 0-coef features: {len(np.where(lasso_model_1se.coef_ == 0)[0])}\")\n",
    "\n",
    "        return {\"lasso_model_1se\": lasso_model_1se, \"lasso_model_min\": lasso_model_min, \"lasso_model_refit\": lm_refit}\n",
    "\n",
    "    def fit_pcr(self):\n",
    "        pcr = PCR()\n",
    "        pcr.fit(self.X_train, self.Y_train)\n",
    "        return {\"pcr\": pcr}\n",
    "\n",
    "class PCR(object):\n",
    "\n",
    "    def __init__(self, num_folds=10):\n",
    "        self.folds = num_folds\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        n, p = X.shape\n",
    "        indices = np.arange(n)\n",
    "        np.random.shuffle(indices)\n",
    "        index_sets = np.array_split(indices, self.folds)\n",
    "        ncomp = min(p, n - 1 - max([len(i) for i in index_sets]))\n",
    "        cv_err = np.zeros(ncomp)\n",
    "\n",
    "        for ifold in range(self.folds):\n",
    "            train_inds =  np.delete(index_sets, obj=ifold, axis=0).ravel()\n",
    "            test_inds = index_sets[ifold]\n",
    "\n",
    "            X_train = X[train_inds, :]\n",
    "            pipeline = Pipeline([('scaling', StandardScaler()), ('pca', PCA())])\n",
    "            pipeline.fit(X_train)\n",
    "            X_train = pipeline.transform(X_train)\n",
    "            coefs = Y[train_inds].T @ X_train / np.sum(X_train**2, axis=0)\n",
    "            b0 = np.mean(Y[train_inds])\n",
    "\n",
    "            X_test = pipeline.transform(X[test_inds, :])\n",
    "\n",
    "            for k in np.arange(ncomp):\n",
    "                preds = X_test[:, :k] @ coefs.T[:k] + b0\n",
    "                cv_err[k] +=  np.sum((Y[test_inds]-preds)**2)\n",
    "\n",
    "        min_ind = np.argmin(cv_err)\n",
    "        self.ncomp = min_ind+1\n",
    "        pipeline = Pipeline([('scaling', StandardScaler()), ('pca', PCA(n_components=self.ncomp))])\n",
    "        self.transform = pipeline.fit(X)\n",
    "        self.model = lm().fit(self.transform.transform(X), Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_ = self.transform.transform(X)\n",
    "        return self.model.predict(X_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e62fc5-9fb2-479f-a93a-bbc56510878e",
   "metadata": {},
   "source": [
    "## Simulation Study Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e37a5-18d7-4768-9dfc-e6400a4e0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"Coding2_Data1.csv\"\n",
    "myData = pd.read_csv(url)\n",
    "Y = myData['Y']\n",
    "X = myData.drop(['Y'], axis = 1)\n",
    "\n",
    "results = run(X, Y, models = [\"linear\", \"ridge\", \"lasso\", \"pcr\"], niter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bdba3-b5b6-4039-a96b-5c91b2ce5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(results).describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bd940-7d16-4914-b51e-5860326fa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [(model, mspe) for model, mspe_values in results.items() for mspe in mspe_values]\n",
    "models, mspe_values = zip(*data_list)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.stripplot(x=models, y=mspe_values, jitter=True, color=\"blue\", size=10)\n",
    "\n",
    "sns.boxplot(x=models, y=mspe_values, boxprops={'alpha': 0.4})\n",
    "plt.title('MSPE Comparison Across Models')\n",
    "plt.ylabel('MSPE')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34257da9-d070-48b9-95f3-af3c76485ccd",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "* Which procedure or procedures yield the best performance in terms of MSPE?\n",
    "  - Lasso Model Refit yields the best performance in terms of MSPE\n",
    "* Conversely, which procedure or procedures show the poorest performance?\n",
    "  - Linear Model yields the poorest performance in terms of MSPE as well as a high relative standard deviation\n",
    "* In the context of Lasso regression, which procedure, Lasso.min or Lasso.1se, yields a better MSPE?\n",
    "  - Both procedures have similar means, but Lasso Model Min has a slightly lower MSPE. Lasso Model 1se has slightly less variability (lower std), however, which means the results are more likely to vary with the addition of training data.\n",
    "* Is refitting advantageous in this case? In other words, does L.Refit outperform Lasso.1se?\n",
    "  - Refitting is advantageous as doing so gives us the best performance out of all the methods, including the lasso 1se/lasso min models\n",
    "* Is variable selection or shrinkage warranted for this particular dataset? To clarify, do you find the performance of the Full model to be comparable to, or divergent from, the best-performing procedure among the other five?\n",
    "  - Yes, it is warranted because it helps simplify the model, and the performance is better in terms of MSPE. With fewer features, the model is more explainable with fewer features, so we can take advantage of the better performance and simpler model with the refitted lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36e39b-e201-4b6d-bff9-b7b9e1869df3",
   "metadata": {},
   "source": [
    "## Simulation Study Case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903a54b-9c2b-43a4-bc0e-6953ac619850",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"Coding2_Data2.csv\"\n",
    "myData = pd.read_csv(url)\n",
    "Y = myData['Y']\n",
    "X = myData.drop(['Y'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd2809-8159-449b-9613-826f5f829dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de577d3-270d-49b5-b70e-0b617313682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run(X, Y, models = [\"ridge\", \"lasso\", \"pcr\"], niter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96c8ae-7f93-4067-b1a4-6751e618f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [(model, mspe) for model, mspe_values in results.items() for mspe in mspe_values]\n",
    "models, mspe_values = zip(*data_list)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.stripplot(x=models, y=mspe_values, jitter=True, color=\"blue\", size=10)\n",
    "\n",
    "sns.boxplot(x=models, y=mspe_values, boxprops={'alpha': 0.4})\n",
    "plt.title('MSPE Comparison Across Models')\n",
    "plt.ylabel('MSPE')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22837c33-0e1e-44fa-8df3-290c50f83dd3",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "* Which procedure or procedures yield the best performance in terms of MSPE?\n",
    "  - The Lasso model refitted had the lowest average MPSE\n",
    "* Conversely, which procedure or procedures show the poorest performance?\n",
    "  - The ridge model had the highest average MPSE\n",
    "* Have you observed any procedure or procedures that performed well in Case I but exhibited poorer performance in Case II, or vice versa? If so, please offer an explanation.\n",
    "   - Ridge regression performed on par with Lasso in the first case study simulation, and decreased in performance in the second case study. This is likely from noise in the additional features in the second case study. Ridge does not actually shrink variables to 0, so added features will result in additional noise in the model if there is no true relationship between some of the added features and the target variable. \n",
    "* Given that Coding2_Data2.csv includes all features found in Coding2_Data1.csv, one might anticipate that the best MSPE in Case II would be equal to or lower than the best MSPE in Case I. Do your simulation results corroborate this expectation? If not, please offer an explanation.\n",
    "  - The simulation does not corroborate this expectation. The refitted lasso model performs the best in both case studies, but it is lower (\\~0.01) in Case study 1 and higher (\\~0.03) in Case study 2. This could be due to different splits of training/test between the two simulations (overfitting issue), or from multicollinearity, where there are relationships between variables making it harder to deduce the true relationships between the features and the targets during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9a505-79b8-4f26-bc7b-15ff202e3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(results).describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
