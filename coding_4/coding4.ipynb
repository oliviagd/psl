{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476363ae",
   "metadata": {},
   "source": [
    "## Coding Assignment 4\n",
    "\n",
    "Team:\n",
    "- Olivia Dalglish (od4)\n",
    "- Arindam Saha (saha2)\n",
    "\n",
    "Contribution: \n",
    "\n",
    "Olivia: Part 1\n",
    "\n",
    "Arindam: Part 2\n",
    "\n",
    "In addition to the above, we discussed our approaches and checked each other's work.\n",
    "\n",
    "### Part I: Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4518d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b269cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "THRESHOLD = 1e-6\n",
    "\n",
    "def Estep(data, G, prob, means, Sigma):\n",
    "    \"\"\"\n",
    "    E-step: compute the responsibility matrix\n",
    "\n",
    "    Parameters:\n",
    "        data (ndarray)\n",
    "        G (int)\n",
    "        prob (ndarray): mixing weights for each Gaussian component\n",
    "        means (ndarray): means of Gaussian components\n",
    "        Sigma (ndarray): cov matrix\n",
    "    \"\"\"\n",
    "    # compute Gaussian PDF values for all data points and components\n",
    "    n, p = data.shape\n",
    "    \n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    det_Sigma = np.linalg.det(Sigma)\n",
    "    norm_factor = 1.0 / np.sqrt((2 * np.pi) ** p * det_Sigma)\n",
    "    \n",
    "    diff = data[:, np.newaxis, :] - means.T\n",
    "    \n",
    "    exponent = -0.5 * np.einsum('nik,kl,nil->ni', diff, Sigma_inv, diff)    \n",
    "    pdf_matrix = norm_factor * np.exp(exponent)\n",
    "    \n",
    "    # multiply by mixing weights\n",
    "    weighted_pdfs = pdf_matrix * prob\n",
    "    \n",
    "    # normalize to get responsibilities\n",
    "    resp = weighted_pdfs / weighted_pdfs.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return resp\n",
    "\n",
    "def Mstep(data, resp):\n",
    "    \"\"\"\n",
    "    M-step: update parameters based on responsibilities\n",
    "\n",
    "    Parameters:\n",
    "        data (ndarray)\n",
    "        resp (ndarray): responsibility matrix from E-step, shape (n, G)\n",
    "    \"\"\"\n",
    "    n, p = data.shape\n",
    "    G = resp.shape[1]\n",
    "\n",
    "    Nk = resp.sum(axis=0)  # sum of responsibilities for each component\n",
    "    prob = Nk / n\n",
    "\n",
    "    means = np.dot(data.T, resp) / Nk\n",
    "\n",
    "    Sigma = np.zeros((p, p))\n",
    "    for k in range(G):\n",
    "        diff = data - means[:, k].T \n",
    "        Sigma += np.dot((resp[:, k][:, np.newaxis] * diff).T, diff)\n",
    "    Sigma /= n\n",
    "\n",
    "    return prob, means, Sigma\n",
    "\n",
    "def loglik(data, G, prob, means, Sigma):\n",
    "    \"\"\"\n",
    "    Compute the log-likelihood of the data given the current parameters of the Gaussian mixture model.\n",
    "    Parameters:\n",
    "        data (ndarray)\n",
    "        G (int)\n",
    "        prob (ndarray): mixing weights for each Gaussian component\n",
    "        means (ndarray): means of Gaussian components\n",
    "        Sigma (ndarray): cov matrix   \n",
    "    \"\"\"\n",
    "    n, p = data.shape\n",
    "    '''\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    Sigma_det = np.linalg.det(Sigma)\n",
    "    normalization_const = 1 / ((2 * np.pi) ** (p / 2) * np.sqrt(Sigma_det))\n",
    "    \n",
    "    # data minus means for each component\n",
    "    diff = data[:, np.newaxis, :] - means.T\n",
    "    \n",
    "    exponent = -0.5 * np.einsum('nkp, pq, nkq -> nk', diff, Sigma_inv, diff)\n",
    "    \n",
    "    # gaussian densities: (n, G)\n",
    "    component_densities = normalization_const * np.exp(exponent)\n",
    "    '''\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    det_Sigma = np.linalg.det(Sigma)\n",
    "    norm_factor = 1.0 / np.sqrt((2 * np.pi) ** p * det_Sigma)\n",
    "    \n",
    "    diff = data[:, np.newaxis, :] - means.T\n",
    "    \n",
    "    exponent = -0.5 * np.einsum('nik,kl,nil->ni', diff, Sigma_inv, diff)    \n",
    "    pdf_matrix = norm_factor * np.exp(exponent)\n",
    "    weighted_densities = pdf_matrix * prob \n",
    "    total_density = np.sum(weighted_densities, axis=1)\n",
    "    log_likelihood = np.sum(np.log(total_density))\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "def myEM(data, G, prob, means, Sigma, itmax=100):\n",
    "    \"\"\"\n",
    "    Runner\n",
    "    \n",
    "    Parameters:\n",
    "        data (ndarray): data, shape (n, p)\n",
    "        G (int): number of Gaussian components\n",
    "        prob (ndarray): initial probability vector\n",
    "        means (ndarray): initial means\n",
    "        Sigma (ndarray): initial cov matrix\n",
    "        itmax (int): maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        prob (ndarray): final probability vector\n",
    "        means (ndarray): final means for each Gaussian component\n",
    "        Sigma (ndarray): final shared cov matrix\n",
    "        loglik (float): final log-likelihood of the model\n",
    "    \"\"\"\n",
    "\n",
    "    log_likelihoods = []\n",
    "\n",
    "    for iteration in range(itmax):\n",
    "        resp = Estep(data, G, prob, means, Sigma)\n",
    "\n",
    "        prob, means, Sigma = Mstep(data, resp)\n",
    "\n",
    "        current_loglik = loglik(data, G, prob, means, Sigma)\n",
    "        log_likelihoods.append(current_loglik)\n",
    "\n",
    "        if iteration > 0 and abs(log_likelihoods[-1] - log_likelihoods[-2]) < THRESHOLD:\n",
    "            break\n",
    "\n",
    "    return prob, means, Sigma, log_likelihoods[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb609bf",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb56c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"faithful.dat\", skiprows=1)[:, 1:]\n",
    "\n",
    "n, p = data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c5db5",
   "metadata": {},
   "source": [
    "##### G = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7481b9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final probabilities\n",
      "[0.04297883 0.95702117]\n",
      "\n",
      "Final means\n",
      "[[ 3.49564188  3.48743016]\n",
      " [76.79789154 70.63205853]]\n",
      "Final cov matrix\n",
      "[[  1.29793612  13.92433626]\n",
      " [ 13.92433626 182.58009247]]\n",
      "log likelihood\n",
      "-1289.5693549424104\n"
     ]
    }
   ],
   "source": [
    "G = 2\n",
    "\n",
    "# initialize mixing weights\n",
    "p1 = 10 / n\n",
    "p2 = 1 - p1\n",
    "prob = np.array([p1, p2])\n",
    "\n",
    "mu1 = np.mean(data[:10], axis=0)\n",
    "mu2 = np.mean(data[10:], axis=0)\n",
    "means = np.column_stack((mu1, mu2))\n",
    "\n",
    "# initialize covariance matrix\n",
    "Sigma = np.zeros((p, p))\n",
    "for i in range(10):\n",
    "    diff = data[i] - mu1\n",
    "    Sigma += np.outer(diff, diff)\n",
    "for i in range(10, n):\n",
    "    diff = data[i] - mu2\n",
    "    Sigma += np.outer(diff, diff)\n",
    "Sigma /= n\n",
    "\n",
    "itmax = 20\n",
    "\n",
    "final_prob, final_means, final_Sigma, log_likelihood = myEM(data, G, prob, means, Sigma, itmax)\n",
    "\n",
    "print(\"Final probabilities\")\n",
    "print(final_prob)\n",
    "\n",
    "print(\"\\nFinal means\")\n",
    "print(final_means)\n",
    "\n",
    "print(\"Final cov matrix\")\n",
    "print(final_Sigma)\n",
    "\n",
    "print(\"log likelihood\")\n",
    "print(log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea1e77",
   "metadata": {},
   "source": [
    "##### G = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68caac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final probabilities\n",
      "[0.04363422 0.07718656 0.87917922]\n",
      "Final means\n",
      "[[ 3.51006918  2.81616674  3.54564083]\n",
      " [77.10563811 63.35752634 71.25084801]]\n",
      "Final cov matrix\n",
      "[[  1.26015772  13.51153756]\n",
      " [ 13.51153756 177.96419105]]\n",
      "log likelihood\n",
      "-1289.350958862739\n"
     ]
    }
   ],
   "source": [
    "G = 3\n",
    "\n",
    "# initialize mixing weights\n",
    "p1 = 10 / n\n",
    "p2 = 20 / n\n",
    "p3 = 1 - p1 - p2\n",
    "prob = np.array([p1, p2, p3])\n",
    "\n",
    "# initialize means\n",
    "mu1 = np.mean(data[:10], axis=0)\n",
    "mu2 = np.mean(data[10:30], axis=0)\n",
    "mu3 = np.mean(data[30:], axis=0)\n",
    "means = np.column_stack((mu1, mu2, mu3))\n",
    "\n",
    "# initialize covariance matrix\n",
    "Sigma = np.zeros((p, p))\n",
    "for i in range(10):\n",
    "    diff = data[i] - mu1\n",
    "    Sigma += np.outer(diff, diff)\n",
    "for i in range(10, 30):\n",
    "    diff = data[i] - mu2\n",
    "    Sigma += np.outer(diff, diff)\n",
    "for i in range(30, n):\n",
    "    diff = data[i] - mu3\n",
    "    Sigma += np.outer(diff, diff)\n",
    "Sigma /= n  # Shape (p, p)\n",
    "\n",
    "itmax = 20\n",
    "\n",
    "final_prob, final_means, final_Sigma, log_likelihood = myEM(data, G, prob, means, Sigma, itmax)\n",
    "\n",
    "print(\"Final probabilities\")\n",
    "print(final_prob)\n",
    "\n",
    "print(\"Final means\")\n",
    "print(final_means)\n",
    "\n",
    "print(\"Final cov matrix\")\n",
    "print(final_Sigma)\n",
    "\n",
    "print(\"log likelihood\")\n",
    "print(log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf66d47",
   "metadata": {},
   "source": [
    "We can see above that our output matches the expected values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc31ceb",
   "metadata": {},
   "source": [
    "### Part II: HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74507758",
   "metadata": {},
   "source": [
    "##### Baum-Welch Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ed9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BW_onestep(data, mx, mz, w, A, B):\n",
    "    # Switching data to be 0-indexed\n",
    "    data = data - 1\n",
    "    n = len(data)\n",
    "\n",
    "    alpha = np.zeros((n, mz))\n",
    "    alpha[0] = w * B[:, data[0]]\n",
    "    for t in range(1, n):\n",
    "        alpha[t] = (alpha[t - 1] @ A) * B[:, data[t]]\n",
    "    \n",
    "    beta = np.zeros((n, mz))\n",
    "    beta[n - 1] = np.ones(mz)\n",
    "    for t in range(n - 2, -1, -1):\n",
    "        beta[t] = A @ (B[:, data[t + 1]] * beta[t + 1])\n",
    "\n",
    "    gamma = np.zeros((n - 1, mz, mz))\n",
    "    for t in range(n - 1):\n",
    "        gamma[t] = alpha[t][:, np.newaxis] * (A * (B[:, data[t + 1]] * beta[t + 1]))\n",
    "\n",
    "    gamma_plus = np.sum(gamma, axis=0)\n",
    "    A_next = gamma_plus / np.sum(gamma_plus, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    gamma_ti = np.vstack((np.sum(gamma, axis=2), alpha[n - 1]))\n",
    "    B_next = np.zeros(B.shape)\n",
    "    for l in range(mx):\n",
    "        t_idxs = np.where(data == l)[0]\n",
    "        B_next[:, l] = np.sum(gamma_ti[t_idxs], axis=0) / np.sum(gamma_ti, axis=0)\n",
    "\n",
    "    return A_next, B_next\n",
    "\n",
    "\n",
    "def myBW(data, mx, mz, w, A, B, niter):\n",
    "    for _ in range(niter):\n",
    "        A, B = BW_onestep(data, mx, mz, w, A, B)\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839072b8",
   "metadata": {},
   "source": [
    "##### Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f898ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myViterbi(data, mx, mz, w, A, B):\n",
    "    # Switching data to be 0-indexed\n",
    "    data = data - 1\n",
    "\n",
    "    # Evaluating probabilities in logarithmic scale to correctly compare really low probabilty values\n",
    "\n",
    "    w = np.log(w)\n",
    "    A = np.log(A)\n",
    "    B = np.log(B)\n",
    "\n",
    "    n = len(data)\n",
    "    delta = np.zeros((n, mz))\n",
    "    delta[0] = w + B[:, data[0]]\n",
    "    for t in range(1, n):\n",
    "        for i in range(mz):\n",
    "            delta[t, i] = np.max(delta[t - 1] + A[:, i]) + B[i, data[t]]\n",
    "\n",
    "    Z = np.zeros(n, dtype=int)\n",
    "    Z[n - 1] = np.argmax(delta[n - 1])\n",
    "    for t in range(n - 2, -1, -1):\n",
    "        Z[t] = np.argmax(delta[t] + A[:, Z[t + 1]])\n",
    "\n",
    "    # Switching Z to be 1-indexed\n",
    "    return Z + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bf11d",
   "metadata": {},
   "source": [
    "##### Testing\n",
    "\n",
    "##### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae4e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: the 2-by-2 transition matrix:\n",
      "[[0.49793938 0.50206062]\n",
      " [0.44883431 0.55116569]]\n",
      "\n",
      "B: the 2-by-3 emission matrix:\n",
      "[[0.22159897 0.20266127 0.57573976]\n",
      " [0.34175148 0.17866665 0.47958186]]\n",
      "\n",
      "Z: [1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1]\n",
      "matches: True\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('Coding4_part2_data.txt').astype(int)\n",
    "\n",
    "mx = 3\n",
    "mz = 2\n",
    "\n",
    "w = np.array([0.5, 0.5])\n",
    "A_init = np.array([\n",
    "    [0.5, 0.5],\n",
    "    [0.5, 0.5],\n",
    "])\n",
    "B_init = np.array([\n",
    "    [1/9, 3/9, 5/9],\n",
    "    [1/6, 2/6, 3/6],\n",
    "])\n",
    "\n",
    "A, B = myBW(data, mx, mz, w, A_init, B_init, 100)\n",
    "\n",
    "print(f'A: the 2-by-2 transition matrix:\\n{A}\\n')\n",
    "print(f'B: the 2-by-3 emission matrix:\\n{B}\\n')\n",
    "\n",
    "\n",
    "Z = myViterbi(data, 3, 2, w, A, B)\n",
    "print(f'Z: {Z}')\n",
    "\n",
    "with open('Coding4_part2_Z.txt') as fh:\n",
    "    expected_Z = np.fromstring(fh.read().strip(), dtype=int, sep= ' ')\n",
    "\n",
    "print(f'matches: {np.all(Z == expected_Z)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9bf06",
   "metadata": {},
   "source": [
    "We can see above that our output matches the expected values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67344c7a",
   "metadata": {},
   "source": [
    "##### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22736d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_20: the 2-by-2 transition matrix:\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\n",
      "B_20: the 2-by-3 emission matrix:\n",
      "[[0.285 0.19  0.525]\n",
      " [0.285 0.19  0.525]]\n",
      "\n",
      "A_100: the 2-by-2 transition matrix:\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\n",
      "B_100: the 2-by-3 emission matrix:\n",
      "[[0.285 0.19  0.525]\n",
      " [0.285 0.19  0.525]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "B_init_same = np.array([\n",
    "    [1/3, 1/3, 1/3],\n",
    "    [1/3, 1/3, 1/3],\n",
    "])\n",
    "A_20, B_20 = myBW(data, mx, mz, w, A_init, B_init_same, 20)\n",
    "A_100, B_100 = myBW(data, mx, mz, w, A_init, B_init_same, 100)\n",
    "\n",
    "print(f'A_20: the 2-by-2 transition matrix:\\n{A_20}\\n')\n",
    "print(f'B_20: the 2-by-3 emission matrix:\\n{B_20}\\n')\n",
    "\n",
    "print(f'A_100: the 2-by-2 transition matrix:\\n{A_100}\\n')\n",
    "print(f'B_100: the 2-by-3 emission matrix:\\n{B_100}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da25afca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.285, 0.19, 0.525)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data == 1), np.mean(data == 2), np.mean(data == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38373551",
   "metadata": {},
   "source": [
    "We can see above that with indistinguishable latent states, Baum-Welch just converges to the probabilities of each value in the data in the emission matrix. This makes sense because it is equivalent to just having one state which emits according to the probabilities found in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
